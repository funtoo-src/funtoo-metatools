#!/usr/bin/env python3

import json
import os
from collections import defaultdict
from concurrent.futures import as_completed
from concurrent.futures.thread import ThreadPoolExecutor

import dyne.org.funtoo.metatools.merge as merge

from metatools.files.release import Kit, SourceCollection, SourceRepository
from metatools.tree import run_shell, GitTree

"""
This file exists to define objects related to the processing of kits and releases. It uses settings defined in 
``merge.model``, and in particular will use ``merge.model.release_yaml`` to access the object hierarchy created
from a release's YAML data.
"""


class KitGenerator:

	"""
	This class represents the work associated with generating a Kit. A ``Kit`` (defined in metatools/files/release.py)
	is passed to the constructor of this object to define settings, and stored within this object as ``self.kit``.

	The KitGenerator takes care of creating or connecting to an existing Git tree that is used to house the results of
	the kit generation, and this Git tree object is stored at ``self.out_tree``.

	The ``self.generate()`` method (and supporting methods) take care of regenerating the Kit. Upon completion,
	``self.kit_sha1`` is set to the SHA1 of the commit containing these updates.
	"""

	kit_sha1 = None
	out_tree = None
	active_repos = set()

	def __init__(self, kit: Kit):
		self.kit = kit

		git_class = merge.model.git_class

		if merge.model.nest_kits:
			root = os.path.join(merge.model.dest_trees, "meta-repo/kits", kit.name)
		else:
			root = os.path.join(merge.model.dest_trees, kit.name)
		self.out_tree = git_class(kit.name, branch=kit.branch, root=root, model=merge.model)
		self.out_tree.initialize()

	async def generate(self):

		"""
		This function will auto-generate a single 'autogenerated' kit by checking out the current version, wiping the
		contents of the git repo, and copying everything over again, updating metadata cache, etc. and then committing (and
		possibly pushing) the result.
		"""

		# load on-disk JSON metadata cache into memory:
		merge.metadata.fetch_kit(self.out_tree)

		steps = [
			merge.steps.CleanTree(),
			merge.steps.GenerateRepoMetadata(self.kit.name, aliases=self.kit.aliases, masters=self.kit.masters, priority=self.kit.priority),
		]
		await self.out_tree.run(steps)
		await self.out_tree.run(self.package_yaml_steps())
		await self.out_tree.run(self.copy_from_fixups_steps())
		await self.out_tree.run([
			merge.steps.RemoveFiles(self.kit.get_excludes()),
			merge.steps.FindAndRemove(["__pycache__"]),
			merge.steps.FindAndRemove(["COPYRIGHT.txt"]), # replaced with COPYRIGHT.rst
			merge.steps.GenerateLicensingFile(text=self.kit.get_copyright_rst()),
			merge.steps.Minify(),
			merge.steps.ELTSymlinkWorkaround(),
			merge.steps.CreateCategories(),
			# TODO: move this to a post-step and only include active licenses.
			# TODO: we should not hard-reference 'gentoo-staging' anymore.
			merge.steps.SyncDir(self.kit.source.repositories["gentoo-staging"].tree.root, "licenses"),
		])

		############################################################################################################
		# We now want to store the md5 digests for the eclasses in the kit at this point and also abort if we have
		# a duplicate eclass:
		############################################################################################################

		merge.model.eclass_hashes.add_eclasses(self.out_tree.root)

		############################################################################################################
		# We will now generate the cached ebuild metadata for this kit, now that all ebuilds and eclasses are in
		# place:
		############################################################################################################

		await self.out_tree.run([merge.steps.GenCache()])

		############################################################################################################
		# Python USE settings auto-generation and other finalization steps:
		############################################################################################################

		post_steps = [
			merge.steps.PruneLicenses()
		] + self.python_auto_use_steps()

		# We can now run all the steps that require access to metadata:

		await self.out_tree.run(post_steps)

		update_msg = "Autogenerated tree updates."
		self.out_tree.gitCommit(message=update_msg, push=merge.model.push)

		# save in-memory metadata cache to JSON:
		merge.metadata.flush_kit(self.out_tree)
		self.kit_sha1 = self.out_tree.head()

	def get_kit_pre_post_steps(self):
		# unhandled steps:
		# TODO: do some forking of profiles to fix this:
		# core/"post": [
		# 					merge.steps.ThirdPartyMirrors(),
		# 					merge.steps.RunSed(["profiles/base/make.defaults"], ["/^PYTHON_TARGETS=/d", "/^PYTHON_SINGLE_TARGET=/d"]),
		# 				],
		# core/pre: merge.steps.SyncDir(merge.model.source_repos["gentoo-staging"].root, "eclass"),
		# merge.steps.SyncFiles(
		# 						merge.model.kit_fixups.context,
		# 						{
		# 							"LICENSE.txt": "LICENSE.txt",
		# 						},
		# 					),
		pass

	def python_auto_use_steps(self):
		"""
		Funtoo and metatools has a feature where we will look at the configured Python kits for the release,
		and auto-generate optimal Python USE settings for each kit in the release. This ensures that things
		can be easily merged without weird Python USE errors. These settings are stored in the following
		location in each kit in the release::

			profiles/funtoo/kits/python-kit/<python-kit-branch>

		When 'ego sync' runs, it will ensure that these settings are automatically enabled based upon what
		your currently-active python-kit is. This means that even if you have multiple python-kit branches
		defined in your release, switching between them is seamless and Python USE settings for all packages
		in the repository will auto-adapt to whatever Python kit is currently enabled.
		"""
		steps = []
		for kit in merge.model.release_yaml.iter_kits(name="python-kit"):
			steps += [merge.steps.GenPythonUse(self.kit.settings, "funtoo/kits/python-kit/%s" % kit.branch)]
		return steps

	def package_yaml_steps(self):
		"""
		This method returns steps required to copy over all 'eclasses' and 'copyfiles' entries in the
		packages.yaml file for the kit, as well as all packages referenced in the 'packages' section
		(from the appropriate source repository.)
		"""

		steps = []
		for repo_name, copyfile_tuples in self.kit.get_individual_files_to_copy().items():
			steps += [merge.steps.CopyFiles(self.kit.source.repositories[repo_name].tree, copyfile_tuples)]

		# Copy over catpkgs listed in 'packages' section:
		for repo_name, packages in self.kit.get_kit_packages():
			self.active_repos.add(repo_name)
			# TODO: add move maps below
			steps += [merge.steps.InsertEbuilds(self.kit.source.repositories[repo_name].tree, skip=None, replace=True, move_maps=None, select=packages, scope=merge.model.release)]
		return steps

	def copy_from_fixups_steps(self):

		# Phase 3: copy eclasses, licenses, profile info, and ebuild/eclass fixups from the kit-fixups repository.

		# First, we are going to process the kit-fixups repository and look for ebuilds and eclasses to replace. Eclasses can be
		# overridden by using the following paths inside kit-fixups:

		# kit-fixups/eclass/1.2-release <--------- global eclasses, get installed to all kits unconditionally for release (overrides those above)
		# kit-fixups/<kit>/global/eclass <-------- global eclasses for a particular kit, goes in all branches (overrides those above)
		# kit-fixups/<kit>/global/profiles <------ global profile info for a particular kit, goes in all branches (overrides those above)
		# kit-fixups/<kit>/<branch>/eclass <------ eclasses to install in just a specific branch of a specific kit (overrides those above)
		# kit-fixups/<kit>/<branch>/profiles <---- profile info to install in just a specific branch of a specific kit (overrides those above)

		# Note that profile repo_name and categories files are excluded from any copying.

		# Ebuilds can be installed to kits by putting them in the following location(s):

		# kit-fixups/<kit>/global/cat/pkg <------- install cat/pkg into all branches of a particular kit
		# kit-fixups/<kit>/<branch>/cat/pkg <----- install cat/pkg into a particular branch of a kit

		# Remember that at this point, we may be missing a lot of eclasses and licenses from Gentoo. We will then perform a final sweep
		# of all catpkgs in the dest_kit and auto-detect missing eclasses from Gentoo and copy them to our dest_kit. Remember that if you
		# need a custom eclass from a third-party overlay, you will need to specify it in the overlay's overlays["ov_name"]["eclasses"]
		# list. Or alternatively you can copy the eclasses you need to kit-fixups and maintain them there :)

		steps = []
		# Here is the core logic that copies all the fix-ups from kit-fixups (eclasses and ebuilds) into place:
		eclass_release_path = "eclass/%s" % merge.model.release
		if os.path.exists(os.path.join(merge.model.kit_fixups.root, eclass_release_path)):
			steps += [merge.steps.SyncDir(merge.model.kit_fixups.root, eclass_release_path, "eclass")]
		fixup_dirs = ["global", "curated", self.kit.branch]
		for fixup_dir in fixup_dirs:
			fixup_path = self.kit.name + "/" + fixup_dir
			if os.path.exists(merge.model.kit_fixups.root + "/" + fixup_path):
				if os.path.exists(merge.model.kit_fixups.root + "/" + fixup_path + "/eclass"):
					steps += [
						merge.steps.InsertFilesFromSubdir(
							merge.model.kit_fixups, "eclass", ".eclass", select="all", skip=None, src_offset=fixup_path
						)
					]
				if os.path.exists(merge.model.kit_fixups.root + "/" + fixup_path + "/licenses"):
					steps += [
						merge.steps.InsertFilesFromSubdir(
							merge.model.kit_fixups, "licenses", None, select="all", skip=None, src_offset=fixup_path
						)
					]
				if os.path.exists(merge.model.kit_fixups.root + "/" + fixup_path + "/profiles"):
					steps += [
						merge.steps.InsertFilesFromSubdir(
							merge.model.kit_fixups, "profiles", None, select="all", skip=["repo_name", "categories"], src_offset=fixup_path
						)
					]
				# copy appropriate kit readme into place:
				readme_path = fixup_path + "/README.rst"
				if os.path.exists(merge.model.kit_fixups.root + "/" + readme_path):
					steps += [merge.steps.SyncFiles(merge.model.kit_fixups.root, {readme_path: "README.rst"})]

				# We now add a step to insert the fixups, and we want to record them as being copied so successive kits
				# don't get this particular catpkg. Assume we may not have all these catpkgs listed in our package-set
				# file...

				steps += [
					merge.steps.InsertEbuilds(merge.model.kit_fixups, ebuildloc=fixup_path, select="all", skip=None, replace=True, scope=merge.model.release)
				]
		return steps



class KitPipeline:

	def __init__(self, key, jobs):
		self.key = key
		self.jobs = jobs

	def initialize_source_repository(self, repo: SourceRepository):
		#if repo_key in merge.model.source_repos:
		#	repo_obj = merge.model.source_repos[repo_key]
		#	if repo_sha1:
		#		repo_obj.gitCheckout(sha1=repo_sha1)
		#	elif repo_branch:
		#		repo_obj.gitCheckout(branch=repo_branch)
		#else:
		merge.model.log.info(f"Initializing Source Repository {repo.name}")
		repo.tree = GitTree(
			repo.name,
			url=repo.url,
			root="%s/%s" % (merge.model.source_trees, repo.name),
			branch=repo.branch,
			commit_sha1=repo.src_sha1,
			origin_check=False,
			reclone=False,
			model=merge.model
		)
		repo.tree.initialize()

	def initialize_sources(self, source_def: SourceCollection):

		"""
		This method initializes the source repositories referenced by the kit to ensure that they are all initialized to the
		proper branch and/or SHA1. Some internal checking is done to avoid re-initializing repositories unnecessarily, so if
		they are already set up properly then no action will be taken.
		"""

		# If we are already using this SourceCollection, no action is needed:
		merge.model.log.info(f"Initializing source collection {source_def.name} with {len(source_def.repositories)} repositories")
		if merge.model.current_source_def == source_def:
			return

		# If we need to switch SourceCollection, we can still avoid unnecessary work:
		# We will go through each of our repositories, and only (re-)initialize it if:
		#
		# 1. Our repo is missing.
		# 2. Our repo exists with same name but is referencing a different branch/sha1.

		repo_futures = []
		with ThreadPoolExecutor(max_workers=4) as executor:
			for repo_name, repo in source_def.repositories.items():
				fut = executor.submit(self.initialize_source_repository, repo)
				repo_futures.append(fut)
			for repo_fut in as_completed(repo_futures):
				# Getting .result() will also cause any exception to be thrown:
				repo_dict = repo_fut.result()
				continue

		merge.model.current_source_def = source_def

	async def run(self):
		pass


class ParallelKitPipeline(KitPipeline):

	async def run(self):
		if not len(self.jobs):
			return
		# All kits here are sharing the same sources collection, so we just have to initialize them once:
		self.initialize_sources(self.jobs[0].kit.source)
		regen_futures = []
		with ThreadPoolExecutor(max_workers=8) as executor:
			for kit_job in self.jobs:
				future = executor.submit(hub.run_async_adapter, kit_job.generate)
				regen_futures.append(future)
			for future in as_completed(regen_futures):
				result = future.result()


class MasterKitPipeline(KitPipeline):

	current_source_def = None

	def __init__(self, jobs):
		super().__init__("masters", jobs)

	async def run(self):
		for kit_job in self.jobs:
			# Each master may have different sources, so perform initialize call prior to each run:
			self.initialize_sources(kit_job.kit.source)
			await kit_job.generate()


class MetaRepoJobController:

	"""
	This class is designed to run the full meta-repo and kit regeneration process -- in other words, the entire
	technical flow of 'merge-kits' when it creates or updates kits and meta-repo.
	"""

	pipeline_count = 0
	kit_pipeline_keys = ["masters"]
	kit_pipeline_slots = defaultdict(list)
	kit_jobs = []

	def __init__(self):
		self.generate_jobs_and_pipelines()

	def iter_pipelines(self):
		for pipeline_key in self.kit_pipeline_keys[1:]:
			yield ParallelKitPipeline(pipeline_key, self.kit_pipeline_slots[pipeline_key])

	async def generate(self):
		merge.metadata.cleanup_error_logs()

		master_pipeline = MasterKitPipeline(jobs=self.kit_pipeline_slots["masters"])
		await master_pipeline.run()

		for pipeline in self.iter_pipelines():
			await pipeline.run()

		# Create meta-repo commit referencing our updated kits:
		merge.kit.generate_metarepo_metadata(merge.model.kit_sha1s)
		merge.model.meta_repo.gitCommit(message="kit updates", skip=["kits"], push=merge.model.push)

		if not merge.model.prod:
			# check out preferred kit branches, because there's a good chance we'll be using it locally.
			for name, ctx in merge.sources.get_kit_preferred_branches().items():
				merge.model.log.info(f"Checking out {name} {ctx.kit.branch}...")
				await merge.kit.checkout_kit(ctx, pull=False)

		if not merge.model.mirror_repos:
			merge.metadata.display_error_summary()
			return

		# Mirroring to GitHub happens here:

		merge.kit.mirror_all_repositories()
		merge.metadata.display_error_summary()

	def get_kit_preferred_branches(self):
		"""
		When we generate a meta-repo, and we're not in "prod" mode, then it's likely that we will be using
		our meta-repo locally. In this case, it's handy to have the proper kits checked out after this is
		done. So for example, we would want gnome-kit 3.36-prime checked out not 3.34-prime, since 3.36-prime
		is the preferred branch in the metadata. This function will return a dict of kit names with the
		values being a AttrDict with the info specific to the kit.
		"""
		out = {}

		for kit_dict in merge.model.kit_groups:
			name = kit_dict["name"]
			stability = kit_dict["stability"]
			if stability != "prime":
				continue
			if name in out:
				# record first instance of kit from the YAML, ignore others (primary kit is the first one listed)
				continue
			out[name] = AttrDict()
			out[name].kit = AttrDict(kit_dict)
		return out

	def create_new_pipeline(self) -> str:
		"""
		This method creates a new kit pipeline and returns its index as a string.
		"""
		pipeline_key = f"pipeline{self.pipeline_count}"
		self.pipeline_count += 1
		self.kit_pipeline_keys.append(pipeline_key)
		return pipeline_key

	def find_existing_pipeline(self, kit_job: KitGenerator):
		"""
		For threading, we want to group kits into collections (pipelines) when they can legally run at the same
		time. This function will help us find a pipeline which we can "legally" join. If we can't find an existing
		pipeline that works for us, this function will return None so we can know to create a new thread pipeline.

		TODO: we can potentially parallelize this further by detecting when there are no source repositories in
		      common, even if ``other.source != kit.source`` -- these can still be auto-generated in parallel.
		"""

		for pipeline_key in self.kit_pipeline_keys[1:]:
			skip_pipeline = False
			for other in self.kit_pipeline_slots[pipeline_key]:
				if other.kit.source != kit_job.kit.source:
					# Autogenerated kit different git sources, can't use this pipeline:
					skip_pipeline = True
					break
				elif other.kit.name == kit_job.kit.name:
					# already processing another branch of same kit in this pipeline, so can't run simultaneously:
					skip_pipeline = True
					break
			if skip_pipeline:
				continue
			return pipeline_key
		return None

	def generate_jobs_and_pipelines(self):

		"""
		This method organizes to-be-generated kits into pipelines containing kit jobs (KitGenerators) to regenerate the kits.

		The first pipeline is called "masters" and is special -- each kit in this pipeline is generated in-order, and there must be only
		one of each of these kits defined in the entire release. These are the 'foundational' kits of the release that contain eclasses and
		that other kits reference (core-kit, llvm-kit).

		Successive pipelines contain one or more kits that can be (re)generated at the same time, because they use the same source repositories.
		To improve performance, this class will parallelize these pipelines.

		For 'regular' kits, we cannot generate two kits in parallel if any of these conditions are true:

		1. The kits are for different branches of the same kit (they will clobber writes to the dest. tree)
		2. The kits reference a different set of source repositories (they need different sha1's checked out at the same time.)
		"""

		all_masters = set()
		for kit in merge.model.release_yaml.iter_kits():
			all_masters |= set(kit.masters)
		for kit in merge.model.release_yaml.iter_kits():
			kit_job = KitGenerator(kit)
			# Keep master list of all kit jobs so we can collect commit data from them after processing is complete:
			self.kit_jobs.append(kit_job)
			if kit.name in all_masters:
				if kit.name in self.kit_pipeline_slots["masters"]:
					raise ValueError(f"This release defines {kit.name} multiple times, but it is a master. Only define one master.")
				self.kit_pipeline_slots["masters"].append(kit_job)
			else:
				my_pipeline = self.find_existing_pipeline(kit_job)
				if my_pipeline is None:
					my_pipeline = self.create_new_pipeline()
				self.kit_pipeline_slots[my_pipeline].append(kit_job)


	# TODO: does this need to be upgraded to handle multiple remotes?
	def mirror_repository(self, repo_obj, base_path):
		"""
		Mirror a repository to its mirror location, ie. GitHub.
		"""

		os.makedirs(base_path, exist_ok=True)
		run_shell(f"git clone --bare {repo_obj.root} {base_path}/{repo_obj.name}.pushme")
		run_shell(
			f"cd {base_path}/{repo_obj.name}.pushme && git remote add upstream {repo_obj.mirror} && git push --mirror upstream"
		)
		run_shell(f"rm -rf {base_path}/{repo_obj.name}.pushme")
		return repo_obj.name

	def mirror_all_repositories(self):
		base_path = os.path.join(merge.temp_path, "mirror_repos")
		run_shell(f"rm -rf {base_path}")
		kit_mirror_futures = []
		with ThreadPoolExecutor(max_workers=8) as executor:
			# Push all kits, then push meta-repo.
			for kit_name, kit_tuple in merge.model.kit_results.items():
				ctx, tree_obj, tree_sha1 = kit_tuple
				future = executor.submit(self.mirror_repository, tree_obj, base_path)
				kit_mirror_futures.append(future)
			for future in as_completed(kit_mirror_futures):
				kit_name = future.result()
				print(f"Mirroring of {kit_name} complete.")
		merge.kit.mirror_repository(merge.model.meta_repo, base_path)
		print("Mirroring of meta-repo complete.")


class MetaRepoGenerator:

	def __init__(self):
		meta_repo_config = self.release_yaml.get_meta_repo_config()
		self.meta_repo = self.git_class(
			name="meta-repo",
			branch=release,
			url=meta_repo_config['url'],
			root=self.dest_trees + "/meta-repo",
			origin_check=True if self.prod else None,
			mirrors=meta_repo_config['mirrors'],
			create_branches=self.create_branches,
			model=self
		)
		self.start_time = datetime.utcnow()
		self.meta_repo.initialize()


# TODO: integrate this into the workflow
def generate_metarepo_metadata(self):
	"""
	Generates the metadata in /var/git/meta-repo/metadata/...
	:param release: the release string, like "1.3-release".
	:param merge.model.meta_repo: the meta-repo GitTree.
	:return: None.
	"""

	if not os.path.exists(merge.model.meta_repo.root + "/metadata"):
		os.makedirs(merge.model.meta_repo.root + "/metadata")

	with open(merge.model.meta_repo.root + "/metadata/kit-sha1.json", "w") as a:
		a.write(json.dumps(output_sha1s, sort_keys=True, indent=4, ensure_ascii=False))

	outf = merge.model.meta_repo.root + "/metadata/kit-info.json"
	all_kit_names = sorted(output_sha1s.keys())

	with open(outf, "w") as a:
		k_info = {}
		out_settings = defaultdict(lambda: defaultdict(dict))
		for kit_dict in merge.model.kit_groups:
			kit_name = kit_dict["name"]
			# specific keywords that can be set for each branch to identify its current quality level
			out_settings[kit_name]["stability"][kit_dict["branch"]] = kit_dict["stability"]
			kind_json = "auto"
			out_settings[kit_name]["type"] = kind_json
		k_info["kit_order"] = all_kit_names
		k_info["kit_settings"] = out_settings

		# auto-generate release-defs. We used to define them manually in foundation:
		rdefs = {}
		for kit_name in all_kit_names:
			rdefs[kit_name] = []
			for def_kit in filter(
				lambda x: x["name"] == kit_name and x["stability"] not in ["deprecated"], merge.model.kit_groups
			):
				rdefs[kit_name].append(def_kit["branch"])

	rel_info = merge.model.release_info()

	k_info["release_defs"] = rdefs
	k_info["release_info"] = rel_info
	a.write(json.dumps(k_info, sort_keys=True, indent=4, ensure_ascii=False))

	with open(merge.model.meta_repo.root + "/metadata/version.json", "w") as a:
		a.write(json.dumps(rel_info, sort_keys=True, indent=4, ensure_ascii=False))





