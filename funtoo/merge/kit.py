#!/usr/bin/env python3

import json
import os
from collections import defaultdict
from concurrent.futures._base import as_completed
from concurrent.futures.thread import ThreadPoolExecutor

import dyne.org.funtoo.metatools.merge as merge

from metatools.files.release import Kit
from metatools.tree import run_shell


class KitJob:
	"""
	This class holds all info relevant to the creation of a specific kit branch from kit.yaml. It contains a
	reference to the definition information in kit-fixups relevant to the kit, as well as the output Git tree
	which will contain our results.
	"""
	kit_sha1 = None
	out_tree = None
	active_repos = set()

	def __init__(self,
	             kit: Kit = None,
	             ):
		self.kit = kit

		git_class = merge.model.git_class

		if merge.model.nest_kits:
			root = os.path.join(merge.model.dest_trees, "meta-repo/kits", kit.name)
		else:
			root = os.path.join(merge.model.dest_trees, kit.name)
		self.out_tree = git_class(kit.name, branch=kit.branch, root=root, model=merge.model)
		self.out_tree.initialize()

	async def generate(self):

		"""
		This function will auto-generate a single 'autogenerated' kit by checking out the current version, wiping the
		contents of the git repo, and copying everything over again, updating metadata cache, etc. and then committing (and
		possibly pushing) the result.
		"""

		# load on-disk JSON metadata cache into memory:
		merge.metadata.fetch_kit(self.out_tree)

		steps = [
			merge.steps.CleanTree(),
			merge.steps.GenerateRepoMetadata(self.kit.name, aliases=self.kit.aliases, masters=self.kit.masters, priority=self.kit.priority),
		] + self.package_yaml_steps() + self.copy_from_fixups_steps() + [
			merge.steps.RemoveFiles(merge.model.get_excludes(self.kit)),
			merge.steps.FindAndRemove(["__pycache__"]),
			merge.steps.FindAndRemove(["COPYRIGHT.txt"]), # replaced with COPYRIGHT.rst
			merge.steps.GenerateLicensingFile(sorted(list(self.active_repos))),
			merge.steps.Minify(),
			merge.steps.ELTSymlinkWorkaround(),
			merge.steps.CreateCategories(),
			# TODO: move this to a post-step and only include active licenses.
			merge.steps.SyncDir(merge.model.source_repos["gentoo-staging"].root, "licenses"),
		]

		await self.out_tree.run(steps)

		############################################################################################################
		# We now want to store the md5 digests for the eclasses in the kit at this point and also abort if we have
		# a duplicate eclass:
		############################################################################################################

		merge.model.eclass_hashes.add_eclasses(self.out_tree.root)

		############################################################################################################
		# We will now generate the cached ebuild metadata for this kit, now that all ebuilds and eclasses are in
		# place:
		############################################################################################################

		await self.out_tree.run([merge.steps.GenCache()])

		############################################################################################################
		# Python USE settings auto-generation and other finalization steps:
		############################################################################################################

		post_steps = [
			merge.steps.PruneLicenses()
		] + self.python_auto_use_steps()

		# We can now run all the steps that require access to metadata:

		await self.out_tree.run(post_steps)

		update_msg = "Autogenerated tree updates."
		self.out_tree.gitCommit(message=update_msg, push=merge.model.push)

		# save in-memory metadata cache to JSON:
		merge.metadata.flush_kit(self.out_tree)
		self.kit_sha1 = self.out_tree.head()

	def get_kit_pre_post_steps(self):
		# unhandled steps:
		# TODO: do some forking of profiles to fix this:
		# core/"post": [
		# 					merge.steps.ThirdPartyMirrors(),
		# 					merge.steps.RunSed(["profiles/base/make.defaults"], ["/^PYTHON_TARGETS=/d", "/^PYTHON_SINGLE_TARGET=/d"]),
		# 				],
		# core/pre: merge.steps.SyncDir(merge.model.source_repos["gentoo-staging"].root, "eclass"),
		# merge.steps.SyncFiles(
		# 						self.kit.kit_fixups.context,
		# 						{
		# 							"LICENSE.txt": "LICENSE.txt",
		# 						},
		# 					),
		pass

	def python_auto_use_steps(self):
		"""
		Funtoo and metatools has a feature where we will look at the configured Python kits for the release,
		and auto-generate optimal Python USE settings for each kit in the release. This ensures that things
		can be easily merged without weird Python USE errors. These settings are stored in the following
		location in each kit in the release::

			profiles/funtoo/kits/python-kit/<python-kit-branch>

		When 'ego sync' runs, it will ensure that these settings are automatically enabled based upon what
		your currently-active python-kit is. This means that even if you have multiple python-kit branches
		defined in your release, switching between them is seamless and Python USE settings for all packages
		in the repository will auto-adapt to whatever Python kit is currently enabled.
		"""
		steps = []
		for kit in merge.model.release_yaml.iter_kits(name="python-kit"):
			steps += [merge.steps.GenPythonUse(self.kit.settings, "funtoo/kits/python-kit/%s" % kit.branch)]
		return steps

	def package_yaml_steps(self):
		"""
		This method returns steps required to copy over all 'eclasses' and 'copyfiles' entries in the
		packages.yaml file for the kit, as well as all packages referenced in the 'packages' section
		(from the appropriate source repository.)
		"""

		steps = []
		for repo_name, copyfile_tuples in merge.model.get_individual_files_to_copy(self.kit).items():
			steps += [merge.steps.CopyFiles(merge.model.source_repos[repo_name], copyfile_tuples)]

		# Copy over catpkgs listed in 'packages' section:
		for repo_name, packages in self.kit.get_kit_packages():
			self.active_repos.add(repo_name)
			from_tree = merge.model.source_repos[repo_name]
			# TODO: add move maps below
			steps += [merge.steps.InsertEbuilds(from_tree, skip=None, replace=True, move_maps=None, select=packages)]
		return steps

	def copy_from_fixups_steps(self):

		# Phase 3: copy eclasses, licenses, profile info, and ebuild/eclass fixups from the kit-fixups repository.

		# First, we are going to process the kit-fixups repository and look for ebuilds and eclasses to replace. Eclasses can be
		# overridden by using the following paths inside kit-fixups:

		# kit-fixups/eclass/1.2-release <--------- global eclasses, get installed to all kits unconditionally for release (overrides those above)
		# kit-fixups/<kit>/global/eclass <-------- global eclasses for a particular kit, goes in all branches (overrides those above)
		# kit-fixups/<kit>/global/profiles <------ global profile info for a particular kit, goes in all branches (overrides those above)
		# kit-fixups/<kit>/<branch>/eclass <------ eclasses to install in just a specific branch of a specific kit (overrides those above)
		# kit-fixups/<kit>/<branch>/profiles <---- profile info to install in just a specific branch of a specific kit (overrides those above)

		# Note that profile repo_name and categories files are excluded from any copying.

		# Ebuilds can be installed to kits by putting them in the following location(s):

		# kit-fixups/<kit>/global/cat/pkg <------- install cat/pkg into all branches of a particular kit
		# kit-fixups/<kit>/<branch>/cat/pkg <----- install cat/pkg into a particular branch of a kit

		# Remember that at this point, we may be missing a lot of eclasses and licenses from Gentoo. We will then perform a final sweep
		# of all catpkgs in the dest_kit and auto-detect missing eclasses from Gentoo and copy them to our dest_kit. Remember that if you
		# need a custom eclass from a third-party overlay, you will need to specify it in the overlay's overlays["ov_name"]["eclasses"]
		# list. Or alternatively you can copy the eclasses you need to kit-fixups and maintain them there :)

		steps = []
		# Here is the core logic that copies all the fix-ups from kit-fixups (eclasses and ebuilds) into place:
		eclass_release_path = "eclass/%s" % merge.model.release
		if os.path.exists(os.path.join(self.kit.kit_fixups.root, eclass_release_path)):
			steps += [merge.steps.SyncDir(self.kit.kit_fixups.root, eclass_release_path, "eclass")]
		fixup_dirs = ["global", "curated", self.kit.branch]
		for fixup_dir in fixup_dirs:
			fixup_path = self.kit.name + "/" + fixup_dir
			if os.path.exists(self.kit.kit_fixups.root + "/" + fixup_path):
				if os.path.exists(self.kit.kit_fixups.root + "/" + fixup_path + "/eclass"):
					steps += [
						merge.steps.InsertFilesFromSubdir(
							merge.model.kit_fixups, "eclass", ".eclass", select="all", skip=None, src_offset=fixup_path
						)
					]
				if os.path.exists(self.kit.kit_fixups.root + "/" + fixup_path + "/licenses"):
					steps += [
						merge.steps.InsertFilesFromSubdir(
							merge.model.kit_fixups, "licenses", None, select="all", skip=None, src_offset=fixup_path
						)
					]
				if os.path.exists(self.kit.kit_fixups.root + "/" + fixup_path + "/profiles"):
					steps += [
						merge.steps.InsertFilesFromSubdir(
							merge.model.kit_fixups, "profiles", None, select="all", skip=["repo_name", "categories"], src_offset=fixup_path
						)
					]
				# copy appropriate kit readme into place:
				readme_path = fixup_path + "/README.rst"
				if os.path.exists(self.kit.kit_fixups.root + "/" + readme_path):
					steps += [merge.steps.SyncFiles(self.kit.kit_fixups.root, {readme_path: "README.rst"})]

				# We now add a step to insert the fixups, and we want to record them as being copied so successive kits
				# don't get this particular catpkg. Assume we may not have all these catpkgs listed in our package-set
				# file...

				steps += [
					merge.steps.InsertEbuilds(merge.model.kit_fixups, ebuildloc=fixup_path, select="all", skip=None, replace=True)
				]
		return steps

# TODO: integrate this into the workflow
def generate_metarepo_metadata(self):
	"""
	Generates the metadata in /var/git/meta-repo/metadata/...
	:param release: the release string, like "1.3-release".
	:param merge.model.meta_repo: the meta-repo GitTree.
	:return: None.
	"""

	if not os.path.exists(merge.model.meta_repo.root + "/metadata"):
		os.makedirs(merge.model.meta_repo.root + "/metadata")

	with open(merge.model.meta_repo.root + "/metadata/kit-sha1.json", "w") as a:
		a.write(json.dumps(output_sha1s, sort_keys=True, indent=4, ensure_ascii=False))

	outf = merge.model.meta_repo.root + "/metadata/kit-info.json"
	all_kit_names = sorted(output_sha1s.keys())

	with open(outf, "w") as a:
		k_info = {}
		out_settings = defaultdict(lambda: defaultdict(dict))
		for kit_dict in merge.model.kit_groups:
			kit_name = kit_dict["name"]
			# specific keywords that can be set for each branch to identify its current quality level
			out_settings[kit_name]["stability"][kit_dict["branch"]] = kit_dict["stability"]
			kind_json = "auto"
			out_settings[kit_name]["type"] = kind_json
		k_info["kit_order"] = all_kit_names
		k_info["kit_settings"] = out_settings

		# auto-generate release-defs. We used to define them manually in foundation:
		rdefs = {}
		for kit_name in all_kit_names:
			rdefs[kit_name] = []
			for def_kit in filter(
				lambda x: x["name"] == kit_name and x["stability"] not in ["deprecated"], merge.model.kit_groups
			):
				rdefs[kit_name].append(def_kit["branch"])

	rel_info = merge.model.release_info()

	k_info["release_defs"] = rdefs
	k_info["release_info"] = rel_info
	a.write(json.dumps(k_info, sort_keys=True, indent=4, ensure_ascii=False))

	with open(merge.model.meta_repo.root + "/metadata/version.json", "w") as a:
		a.write(json.dumps(rel_info, sort_keys=True, indent=4, ensure_ascii=False))


#TODO: does this need to be upgraded to handle multiple remotes?
def mirror_repository(repo_obj, base_path):
	"""
	Mirror a repository to its mirror location, ie. GitHub.
	"""

	os.makedirs(base_path, exist_ok=True)
	run_shell(f"git clone --bare {repo_obj.root} {base_path}/{repo_obj.name}.pushme")
	run_shell(
		f"cd {base_path}/{repo_obj.name}.pushme && git remote add upstream {repo_obj.mirror} && git push --mirror upstream"
	)
	run_shell(f"rm -rf {base_path}/{repo_obj.name}.pushme")
	return repo_obj.name


def mirror_all_repositories():
	base_path = os.path.join(merge.temp_path, "mirror_repos")
	run_shell(f"rm -rf {base_path}")
	kit_mirror_futures = []
	with ThreadPoolExecutor(max_workers=8) as executor:
		# Push all kits, then push meta-repo.
		for kit_name, kit_tuple in merge.model.kit_results.items():
			ctx, tree_obj, tree_sha1 = kit_tuple
			future = executor.submit(mirror_repository, tree_obj, base_path)
			kit_mirror_futures.append(future)
		for future in as_completed(kit_mirror_futures):
			kit_name = future.result()
			print(f"Mirroring of {kit_name} complete.")
	merge.kit.mirror_repository(merge.model.meta_repo, base_path)
	print("Mirroring of meta-repo complete.")
