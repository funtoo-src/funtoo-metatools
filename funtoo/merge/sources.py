#!/usr/bin/env python3

import logging
from collections import defaultdict
from concurrent.futures import as_completed
from concurrent.futures.thread import ThreadPoolExecutor

import dyne.org.funtoo.metatools.merge as merge
from dict_tools.data import NamespaceDict

from metatools.tree import GitTree


class Kit:
	def __init__(self, kind=None, name=None):
		self.kind = kind
		self.name = name


class SourceRepository:

	def __init__(self, name=None, url=None, branch="master", src_sha1=None):
		logging.warning(f"Going to initialize/git fetch for {name}")
		self.name = name
		self.url = url
		self.branch = branch
		self.src_sha1 = src_sha1
		#if repo_key in merge.model.source_repos:
		#	repo_obj = merge.model.source_repos[repo_key]
		#	if repo_sha1:
		#		repo_obj.gitCheckout(sha1=repo_sha1)
		#	elif repo_branch:
		#		repo_obj.gitCheckout(branch=repo_branch)
		#else:

		repo_obj = GitTree(
			name,
			url=url,
			root="%s/%s" % (merge.model.source_trees, name),
			branch=branch,
			commit_sha1=src_sha1,
			origin_check=False,
			reclone=False,
			model=merge.model
		)
		repo_obj.initialize()


class SourceRepositoryCollection:
	# TODO: complete this and fix constructor
	def __init__(self, source):
		repos = list(merge.model.get_repos(source))
		repo_futures = []
		with ThreadPoolExecutor(max_workers=1) as executor:
			for repo_dict in repos:
				# TODO: this should create a new SourceRepository object:
				fut = executor.submit(initialize_repo, repo_dict)
				repo_futures.append(fut)
			for repo_fut in as_completed(repo_futures):
				# Getting .result() will also cause any exception to be thrown:
				repo_dict = repo_fut.result()
				continue
		merge.model.current_source_def = source


def get_kit_preferred_branches():
	"""
	When we generate a meta-repo, and we're not in "prod" mode, then it's likely that we will be using
	our meta-repo locally. In this case, it's handy to have the proper kits checked out after this is
	done. So for example, we would want gnome-kit 3.36-prime checked out not 3.34-prime, since 3.36-prime
	is the preferred branch in the metadata. This function will return a dict of kit names with the
	values being a NamespaceDict with the info specific to the kit.
	"""
	out = {}

	for kit_dict in merge.model.kit_groups:
		name = kit_dict["name"]
		stability = kit_dict["stability"]
		if stability != "prime":
			continue
		if name in out:
			# record first instance of kit from the YAML, ignore others (primary kit is the first one listed)
			continue
		out[name] = NamespaceDict()
		out[name].kit = NamespaceDict(kit_dict)
	return out


def get_kits_in_correct_processing_order():

	"""
	Kits should be processed in a certain order. In the old days, this was done linearly, one at a time.
	In the new code base, we support multi-threading, so things can run in parallel. But there are still
	rules. Here's how the new system works:

	1. core-kit gets processed (generated) first. This is because it contains the official set of
	   eclasses and we need this set of eclasses for generation of the metadata cache for all the
	   other kits.

	2. Then, we order kits into 'pipelines'. All the kits in a pipeline can run in parallel.

	# TODO: instantiate classes and return something cleaner than what is returned here.

	This function returns two objects. The first is a list called `pipeline_keys`. This is a list of
	names of pipelines, and the pipelines should be processed in the order listed in
	`pipeline_keys`. `core-kit` is in its own pipeline by itself so it gets processed first.

	The second variable returned is a dictionary called `pipelines` which maps the pipeline name to a
	list of kits. Actually, a list of kit-dict definitions (YAML definitions).
	"""

	# Rules:
	#
	# 1. If kits don't have the same source definitions (source repos), they can't be in the same
	#    thread-group.
	#
	# 2. If kits have the same name but different branches, they can't be in the same thread group
	#    (We can't have two threads messing with the same git repo at the same time.)
	#
	# 3. Core-kit must be processed first and have only one branch defined.

	kit_pipeline_keys = ["core-kit"]
	kit_pipeline_slots = defaultdict(list)
	pipeline_count = 0

	def find_existing_pipeline(kit: Kit):
		"""
		For threading, we want to group kits into collections (pipelines) when they can legally run at the same
		time. This function will help us find a pipeline which we can "legally" join. If we can't find an existing
		pipeline that works for us, this function will return None so we can know to create a new thread pipeline.
		"""

		for pipeline_key in kit_pipeline_keys[1:]:
			skip_pipeline = False
			for other in kit_pipeline_slots[pipeline_key]:
				if other.source != kit.source:
					# Autogenerated kit different git sources, can't use this pipeline:
					skip_pipeline = True
					break
				elif other.name == kit.name:
					# already processing another branch of same kit in this pipeline, so can't run simultaneously:
					skip_pipeline = True
					break
			if skip_pipeline:
				continue
			return pipeline_key
		return None

	for kit_dict in merge.model.kit_groups:
		if kit_dict["kind"] == "independent":
			raise NotImplementedError()
		kit = Kit(**kit_dict)

		if kit.name == "core-kit":
			if len(kit_pipeline_slots["core-kit"]):
				raise ValueError("You must only define one core-kit in your kit groups.")
			kit_pipeline_slots["core-kit"].append(kit)
		else:
			pipeline_key = find_existing_pipeline(kit)
			if pipeline_key is None:
				pipeline_key = f"pipeline{pipeline_count}"
				pipeline_count += 1
				kit_pipeline_keys.append(pipeline_key)
			kit_pipeline_slots[pipeline_key].append(kit_dict)

	logging.info(f"{len(kit_pipeline_keys)} pipelines generated:")
	for key in kit_pipeline_keys:
		logging.info(f" == {key}")
		for kit_dict in kit_pipeline_slots[key]:
			logging.info(f"  * {kit_dict['name']} / {kit_dict['branch']} / {kit_dict['kind']}")
	return kit_pipeline_keys, kit_pipeline_slots
