#!/usr/bin/env python3

# So, this is the new distfile spider. What I want to do is to have this thing be able to load
# metadata JSON and then check with mongodb to ensure files are in fastpull, and if not, run through
# the JSON and try to grab missing files, and generate a report. Then exit. So it isn't a long-
# running daemon -- it will run for as long as needed to do its thing.
import asyncio
import logging
import os
import sys
from argparse import ArgumentParser
from datetime import datetime

from merge_utils.hub import Hub


def expand_uris(src_uri_list):
	real_uri = []
	for src_uri in src_uri_list:
		if src_uri.startswith("mirror://"):
			real_uri.append(hub.merge.metadata.expand_thirdpartymirror(tp_mirrors, src_uri))
		else:
			slash_split = src_uri.split("/")
			if len(slash_split) == 0:
				continue
			elif slash_split[0] not in ["http:", "https:", "ftp:"]:
				continue
			real_uri.append(src_uri)
	return real_uri


async def do_distfile_fetch(file, datums):
	"""
	This function takes a "file" which is the JSON structure from a kit-cache JSON that describes a file associated
	with a package. It will then see if the file is in the fastpull DB. If not, it will ensure the file is fetched,
	which will then cause it to be inserted into the fastpull DB.
	"""
	sha512 = file["hashes"]["sha512"]
	size = int(file["size"])
	fp_path = hub.merge.fastpull.get_disk_path(sha512)
	if not os.path.exists(fp_path):
		# TODO: the file could still be locally fetched. So we may still be able to inject? Likely just a weird corner case:
		src_uris = expand_uris(file["src_uri"])
		if not len(src_uris):
			return
		try:
			a = hub.pkgtools.ebuild.Artifact(url=src_uris[0], final_name=file["name"], expect={"sha512" : sha512, "size": size})
		except hub.pkgtools.ebuild.DigestFailure as af:
			logging.error(af)
			return
		print(a.final_path)
		await hub.merge.fastpull.inject_into_fastpull(a)
	else:
		# we still may have a missing database entry. And we should fix this. In this conditional block, the file
		# exists in fastpull, but we do not have a DB entry for it. We now know that we should create one. So
		# let's do it.
	existing = hub.FASTPULL.find_one({"filename" : file["name"], "sha512" : file["hashes"]["sha512"]})
	if existing:
		pass
	else:
		db_entry = {}
		db_entry["fetched_on"] = datetime.utcnow()
		db_entry["filename"] = file["name"]
		db_entry["hashes"] = file["hashes"]
		db_entry["size"] = size
		db_entry["src_uri"] = file["src_uri"]
		db_entry["refs"] = [ {
			"kit": datums["kit"],
			"catpkg": datums["catpkg"]
		}]
		hub.FASTPULL.insert_one(db_entry)


	# TODO: I think I had some locations where I assumed size of file was bundled with hashes, but that is not how it
	# is in the JSON.

async def main_thread():
	mk_json = hub.merge.metadata.load_json(args.infile)
	futures = []
	for atom, datums in mk_json["atoms"].items():
		if "files" in datums:
			for file in datums["files"]:
				if "hashes" in file and "sha512" in file["hashes"]:
					future = asyncio.Task(do_distfile_fetch(file, datums))
					futures.append(future)
	results, exceptions = await hub.pkgtools.autogen.gather_pending_tasks(futures)
	if len(exceptions):
		print("Error: the following exceptions were encountered:")
		for x in exceptions:
			print(x)
		sys.exit(1)



if __name__ == "__main__":

	hub = Hub()
	hub.add("funtoo/merge")
	hub.add("funtoo/pkgtools")

	ap = ArgumentParser()
	ap.add_argument("infile")
	args = ap.parse_args()

	# We need kit-fixups available so we can grab the thirdpartymirrors file from it.

	hub.FIXUP_REPO = hub.merge.tree.GitTree(
		"kit-fixups",
		hub.MERGE_CONFIG.branch("kit-fixups"),
		url=hub.MERGE_CONFIG.kit_fixups,
		root=hub.MERGE_CONFIG.source_trees + "/kit-fixups",
	)
	hub.FIXUP_REPO.initialize()

	tp_mirrors = hub.merge.metadata.get_thirdpartymirrors(
		os.path.expanduser("~/repo_tmp/dest-trees/meta-repo/kits/core-kit")
	)

	hub.LOOP.run_until_complete(main_thread())

# vim: ts=4 sw=4 noet
