#!/usr/bin/python3

# The purpose of this executable is to point to a directory path with existing distfiles, and grab them all and
# add them to fastpull.

# TODO: current architecture involves running merge-kits and then distfile-spider all on a separate system. But
#       this can create issues with files that are periodically synced such as vivaldi and vim, which are not
#       always current. This means we may want a different architecture here where we remotely send data to
#       the distfile spider and it works in a "dumb" capacity to do what it's told.

import asyncio
import logging
import multiprocessing
import os
import sys
from asyncio import Queue, Task, FIRST_COMPLETED
from concurrent.futures._base import as_completed
from concurrent.futures.thread import ThreadPoolExecutor
from queue import Queue

import pop.hub
from merge_utils.config import Configuration

hub_logger = logging.getLogger("pop.hub")
hub_logger.setLevel(logging.DEBUG)
hub = pop.hub.Hub()

num_indexers = multiprocessing.cpu_count()
main_queue = Queue(maxsize=1000)
terminus = "ALL_DONE_WITH_WORK"


def file_walker(path):
	for root, dirs, files in os.walk(path, topdown=False):
		for name in files:
			main_queue.put(os.path.join(root, name))
	for count in range(0, num_indexers):
		main_queue.put(terminus)


def file_indexer():
	while True:
		next_file = main_queue.get()
		if next_file == terminus:
			break
		print(next_file)
		hub.pkgtools.fastpull.inject_into_fastpull(next_file, symlink=True)


async def distfile_queuer():
	size_exists = 0
	size_todo = 0
	tp_mirrors = hub.merge.metadata.get_thirdpartymirrors(
		os.path.expanduser("~/repo_tmp/dest-trees/meta-repo/kits/core-kit")
	)
	for pkg in hub.DEEPDIVE.find({}):
		if pkg["kit"] in ["games-kit", "nokit"]:
			continue
		if "files" in pkg:
			for file in pkg["files"]:
				if "src_uri" in file:
					if "digests" in file:
						# compat with deepdive change
						file["hashes"] = file["digests"]
					if "size" in file and "sha512" in file["hashes"]:
						sz_bytes = int(file["size"])
						dp = hub.pkgtools.fastpull.get_disk_path(final_data=file)
						if os.path.exists(dp):
							size_exists += sz_bytes
							continue
						else:
							size_todo += sz_bytes
					else:
						# skip
						continue
					if file["src_uri"][0].startswith("mirror://"):
						real_uri = hub.merge.metadata.expand_thirdpartymirror(tp_mirrors, file["src_uri"][0])
						if real_uri is None:
							print(f"{file['src_uri'][0]} expanded to None, skipping.")
							continue
					else:
						real_uri = file["src_uri"][0]
					slash_split = real_uri.split("/")
					if len(slash_split) == 0:
						# invalid, continue
						print(f"Invalid, skipping. {real_uri}")
						continue
					elif slash_split[0] not in ["http:", "https:", "ftp:"]:
						print(f"URL invalid, skipping: {real_uri}")
						continue
					a = hub.pkgtools.ebuild.Artifact(url=real_uri, final_name=file["name"])
					if not a.is_fetched():
						print(real_uri, file["name"])
						#await a.ensure_fetched()

	print(f"{size_exists} bytes existing.")
	print(f"{size_todo} bytes to download.")

if __name__ == "__main__":
	hub.pop.sub.add("funtoo.merge")
	hub.pop.sub.add("funtoo.pkgtools", omit_class=False)
	hub.MERGE_CONFIG = config = Configuration()
	hub.pkgtools.autogen.load_autogen_config()
	loop = asyncio.get_event_loop()
	loop.run_until_complete(distfile_queuer())


# vim: ts=4 sw=4 noet
