#!/usr/bin/python3

# The purpose of this executable is to point to a directory path with existing distfiles, and grab them all and
# add them to fastpull.

# TODO: current architecture involves running merge-kits and then distfile-spider all on a separate system. But
#       this can create issues with files that are periodically synced such as vivaldi and vim, which are not
#       always current. This means we may want a different architecture here where we remotely send data to
#       the distfile spider and it works in a "dumb" capacity to do what it's told.

import asyncio
import logging
import multiprocessing
import os
import sys
from asyncio import Task
from concurrent.futures._base import as_completed
from concurrent.futures.thread import ThreadPoolExecutor
from queue import Queue

import pop.hub
from merge_utils.config import Configuration

hub_logger = logging.getLogger("pop.hub")
hub_logger.setLevel(logging.DEBUG)
hub = pop.hub.Hub()

num_indexers = multiprocessing.cpu_count()
main_queue = Queue(maxsize=1000)
terminus = "ALL_DONE_WITH_WORK"

def file_walker(path):
	for root, dirs, files in os.walk(path, topdown=False):
		for name in files:
			main_queue.put(os.path.join(root,name))
	for count in range(0, num_indexers):
		main_queue.put(terminus)

def file_indexer():
	while True:
		next_file = main_queue.get()
		if next_file == terminus:
			break
		print(next_file)
		hub.pkgtools.fastpull.inject_into_fastpull(next_file, symlink=True)



async def distfile_queuer():
	size_exists = 0
	size_todo = 0
	for pkg in hub.DEEPDIVE.find({}):
		if pkg["kit"] in [ "games-kit", "nokit"]:
			continue
		print(pkg["kit"])
		if "files" in pkg:
			for file in pkg["files"]:
				if "src_uri" in file:
					if "digests" in file:
						# compat with deepdive change
						file["hashes"] = file["digests"]
					if "size" in file and "sha512" in file["hashes"]:
						sz_bytes = int(file["size"])
						dp = hub.pkgtools.fastpull.get_disk_path(final_data=file)
						if os.path.exists(dp):
							size_exists += sz_bytes
							continue
						else:
							size_todo += sz_bytes
					else:
						# skip
						continue
					if file["src_uri"][0].startswith("mirror://"):
						# skip for now
						continue
					slash_split = file["src_uri"][0].split("/")
					if len(slash_split) == 0:
						# invalid, continue
						continue
					elif slash_split[0] not in [ "http:", "https:", "ftp:" ]:
						continue
					a = hub.pkgtools.ebuild.Artifact(url = file["src_uri"][0], final_name=file["name"])
					if not a.is_fetched():
						yield a
	print(f"{size_exists} bytes existing.")
	print(f"{size_todo} bytes to download.")

	#futures = []
	#with ThreadPoolExecutor() as executor:
	#	futures.append(executor.submit(file_walker, path))
	#	for count in range(0, num_indexers):
	#		futures.append(executor.submit(file_indexer))
#
#		for future in as_completed(futures):
#			sys.stdout.write("x")
#			sys.stdout.flush()
#		print("Indexing complete.")

async def main_thread():
	tasks = []
	# create tons of tasks
	async for a in distfile_queuer():
		tasks.append(Task(a.ensure_fetched()))
	completion_list = hub.pkgtools.autogen.gather_pending_tasks(tasks)


if __name__ == "__main__":
	hub.pop.sub.add("funtoo.merge")
	hub.pop.sub.add("funtoo.pkgtools", omit_class=False)
	hub.MERGE_CONFIG = config = Configuration()
	hub.pkgtools.autogen.load_autogen_config()
	loop = asyncio.get_event_loop()
	loop.run_until_complete(main_thread())
