#!/usr/bin/python3

# The purpose of this executable is to point to a directory path with existing distfiles, and grab them all and
# add them to fastpull.

# TODO: current architecture involves running merge-kits and then distfile-spider all on a separate system. But
#       this can create issues with files that are periodically synced such as vivaldi and vim, which are not
#       always current. This means we may want a different architecture here where we remotely send data to
#       the distfile spider and it works in a "dumb" capacity to do what it's told.

import asyncio
import logging
import multiprocessing
import os
import sys
from asyncio import Queue, Task, FIRST_COMPLETED
from concurrent.futures._base import as_completed
from concurrent.futures.thread import ThreadPoolExecutor
from queue import Queue

import pop.hub
from dict_tools.data import NamespaceDict

from merge_utils.config import Configuration
from merge_utils.tree import GitTree

hub_logger = logging.getLogger("pop.hub")
hub_logger.setLevel(logging.DEBUG)
hub = pop.hub.Hub()

num_indexers = multiprocessing.cpu_count()
main_queue = Queue(maxsize=1000)
terminus = "ALL_DONE_WITH_WORK"


def file_walker(path):
	for root, dirs, files in os.walk(path, topdown=False):
		for name in files:
			main_queue.put(os.path.join(root, name))
	for count in range(0, num_indexers):
		main_queue.put(terminus)


def file_indexer():
	while True:
		next_file = main_queue.get()
		if next_file == terminus:
			break
		print(next_file)
		hub.pkgtools.fastpull.inject_into_fastpull(next_file, symlink=True)


async def do_distfile_fetch(file):
	if file["src_uri"][0].startswith("mirror://"):
		real_uri = hub.merge.metadata.expand_thirdpartymirror(tp_mirrors, file["src_uri"][0])
		if real_uri is None:
			print(f"{file['src_uri'][0]} expanded to None, skipping.")
			return
	else:
		real_uri = file["src_uri"][0]
	slash_split = real_uri.split("/")
	if len(slash_split) == 0:
		# invalid, continue
		print(f"Invalid, skipping. {real_uri}")
		return
	elif slash_split[0] not in ["http:", "https:", "ftp:"]:
		print(f"URL invalid, skipping: {real_uri}")
		return
	a = hub.pkgtools.ebuild.Artifact(url=real_uri, final_name=file["name"])
	if not a.is_fetched():
		print(real_uri, file["name"])


def sizeof_fmt(num, suffix="B"):
	for unit in ["", "Ki", "Mi", "Gi", "Ti", "Pi", "Ei", "Zi"]:
		if abs(num) < 1024.0:
			return "%3.1f%s%s" % (num, unit, suffix)
		num /= 1024.0
	return "%.1f%s%s" % (num, "Yi", suffix)


def print_summary(stats, kit=None, full=False):
	if kit is not None:
		print(f"* Kit {kit.name} branch {kit.branch}:")
	else:
		print("* Global Statistics:")
	print(f"      Total bytes: {sizeof_fmt(stats.bytes_exist + stats.bytes_todo)}")
	print(f"      Total files: {stats.files_exist + stats.files_todo}")
	print(f"   Skipped digest: {len(stats.skip.no_digest)}")
	print(f"  Skipped src_uri: {len(stats.skip.no_src_uri)}")
	print(f"     Skipped size: {len(stats.skip.no_size)}")
	print(f"   Skipped sha512: {len(stats.skip.no_sha512)}")
	print()


def create_stats():
	return NamespaceDict(
		{
			"bytes_exist": 0,
			"bytes_todo": 0,
			"files_exist": 0,
			"files_todo": 0,
			"skip": NamespaceDict({"no_digest": set(), "no_src_uri": set(), "no_size": set(), "no_sha512": set()}),
		}
	)


def add_stats(main_stats, new_stats):
	for key in ["bytes_exist", "bytes_todo", "files_exist", "files_todo"]:
		main_stats[key] += new_stats[key]
	for skip_key in ["no_digest", "no_src_uri", "no_size", "no_sha512"]:
		main_stats.skip[key] = main_stats.skip[skip_key] | new_stats.skip[skip_key]


async def distfile_queuer():
	global_stats = create_stats()
	for kit_group in hub.KIT_GROUPS:
		ctx = NamespaceDict()
		ctx["kit"] = kit = NamespaceDict(kit_group)
		ctx["stats"] = stats = create_stats()
		for pkg in hub.DEEPDIVE.find({"kit": kit.name, "branch": kit.branch}):
			if "files" in pkg:
				for file in pkg["files"]:

					if "src_uri" not in file:
						stats.skip.no_src_uri.add(pkg["atom"])
						continue

					if not "digests" in file and not "hashes" in file:
						stats.skip.no_digest.add(pkg["atom"])
						continue

					if "digests" in file:
						# compat with deepdive change
						file["hashes"] = file["digests"]

					if not "size" in file:
						stats.skip.no_size.add(pkg["atom"])
						continue

					if not "sha512" in file["hashes"]:
						stats.skip.no_sha512.add(pkg["atom"])
						continue

					sz_bytes = int(file["size"])
					dp = hub.pkgtools.fastpull.get_disk_path(final_data=file)
					if os.path.exists(dp):
						stats.bytes_exist += sz_bytes
						stats.files_exist += 1
					else:
						stats.bytes_todo += sz_bytes
						stats.files_todo += 1
		if ctx.kit.name == "perl-kit":
			print_summary(ctx.stats, ctx.kit, full=True)
		else:
			print_summary(ctx.stats, ctx.kit)
		add_stats(global_stats, stats)
	print_summary(global_stats)


if __name__ == "__main__":
	hub.MERGE_CONFIG = config = Configuration()
	hub.RELEASE = "1.4-release"
	hub.FIXUP_REPO = GitTree(
		hub, "kit-fixups", config.branch("kit-fixups"), url=config.kit_fixups, root=config.source_trees + "/kit-fixups",
	)

	hub.pop.sub.add("funtoo.cache")
	hub.pop.sub.add("funtoo.merge")
	hub.pop.sub.add("funtoo.pkgtools", omit_class=False)

	hub.pkgtools.autogen.load_autogen_config()
	hub.FIXUP_REPO.initialize()
	hub.KIT_GROUPS = list(hub.merge.foundations.kit_groups())
	tp_mirrors = hub.merge.metadata.get_thirdpartymirrors(
		os.path.expanduser("~/repo_tmp/dest-trees/meta-repo/kits/core-kit")
	)

	for group in hub.KIT_GROUPS:
		print("GROUPY", group)

	loop = asyncio.get_event_loop()

	loop.run_until_complete(distfile_queuer())


# vim: ts=4 sw=4 noet
