#!/usr/bin/python3

# The purpose of this executable is to point to a directory path with existing distfiles, and grab them all and
# add them to fastpull.
import asyncio
import logging
import multiprocessing
import os
import sys
from concurrent.futures._base import as_completed
from concurrent.futures.thread import ThreadPoolExecutor
from queue import Queue

import pop.hub
from merge_utils.config import Configuration

hub_logger = logging.getLogger("pop.hub")
hub_logger.setLevel(logging.DEBUG)
hub = pop.hub.Hub()

num_indexers = multiprocessing.cpu_count()
main_queue = Queue(maxsize=1000)
terminus = "ALL_DONE_WITH_WORK"

def file_walker(path):
	for root, dirs, files in os.walk(path, topdown=False):
		for name in files:
			main_queue.put(os.path.join(root,name))
	for count in range(0, num_indexers):
		main_queue.put(terminus)

def file_indexer():
	while True:
		next_file = main_queue.get()
		if next_file == terminus:
			break
		print(next_file)
		hub.pkgtools.fastpull.inject_into_fastpull(next_file, symlink=True)



async def main_thread():
	size_exists =0
	size_todo = 0
	for pkg in hub.DEEPDIVE.find({}):
		if pkg["kit"] == "nokit":
			continue
		print(pkg["kit"])
		if "files" in pkg:
			for file in pkg["files"]:
				if "src_uri" in file:
					if "digests" in file:
						# compat with deepdive change
						file["hashes"] = file["digests"]
					if "size" in file and "sha512" in file["hashes"]:
						sz_bytes = int(file["size"])
						dp = hub.pkgtools.fastpull.get_disk_path(final_data=file)
						if os.path.exists(dp):
							size_exists += sz_bytes
							continue
						else:
							size_todo += sz_bytes
					else:
						# skip
						continue
					if file["src_uri"][0].startswith("mirror://"):
						# skip for now
						continue
					a = hub.pkgtools.ebuild.Artifact(url = file["src_uri"][0], final_name=file["name"])
					await a.ensure_fetched()
	print(f"{size_exists} bytes existing.")
	print(f"{size_todo} bytes to download.")

	#futures = []
	#with ThreadPoolExecutor() as executor:
	#	futures.append(executor.submit(file_walker, path))
	#	for count in range(0, num_indexers):
	#		futures.append(executor.submit(file_indexer))
#
#		for future in as_completed(futures):
#			sys.stdout.write("x")
#			sys.stdout.flush()
#		print("Indexing complete.")


if __name__ == "__main__":
	hub.pop.sub.add("funtoo.merge")
	hub.pop.sub.add("funtoo.pkgtools")
	hub.MERGE_CONFIG = config = Configuration()
	loop = asyncio.get_event_loop()
	loop.set_debug(enabled=True)
	loop.run_until_complete(main_thread())
